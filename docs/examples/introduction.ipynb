{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Introduction to the MetObs-toolkit\n",
    "\n",
    "In this introduction, you will learn the principal components and methods in the MetObs-toolkit. Let's start by importing it.\n",
    "\n",
    "Since this package is under development, it is often relevant to know the precise version of the toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metobs_toolkit\n",
    "\n",
    "#Print out the version of the toolkit\n",
    "print(metobs_toolkit.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## The Dataset class\n",
    "\n",
    "The ´´Dataset´´ class is for most applications the most important class. It holds all your stations and it's data. Thus a ´´Dataset´´ is in principal a collection of stations.\n",
    "\n",
    "Since raw data files often include observations from multiple stations, we import our raw data always directly into a ´´Dataset´´. We use the ´´Dataset.import_data_from_file()´´ method, to import the raw data into a Dataset. \n",
    "\n",
    "A key component for importing raw data, is a description of what your data represents and how it is formatted. This is done by providing a **template file**, that describes how your raw data is structured. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Importing your raw data\n",
    "\n",
    "As an example we will import a demo file of raw observations. In order to do that we need to :\n",
    "\n",
    "* Create a template file for our raw data file. The ´build_template_prompt´ function will guide you in this process. It will ask questions, once you answerd them a template file is created. It will also propose some code that you use to import your data\n",
    "* Create a ´Dataset` instance \n",
    "* Add the raw data into the ´Dataset´."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your raw data file (we use the demo file as example)\n",
    "path_to_datafile=metobs_toolkit.demo_datafile\n",
    "\n",
    "# We will also use a metadata file\n",
    "path_to_metadatafile=metobs_toolkit.demo_metadatafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script true\n",
    "\n",
    "#Create a template for these data files\n",
    "metobs_toolkit.build_template_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the path to the templatefile that was created\n",
    "path_to_templatefile=metobs_toolkit.demo_template #demo file as example!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Now that we have the datafiles and the templatefile, we create an empty ´Dataset´, and import the data into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = metobs_toolkit.Dataset() #Create a new dataset object\n",
    "\n",
    "#Load the data\n",
    "dataset.import_data_from_file(\n",
    "                    template_file=path_to_templatefile, #The template file\n",
    "                    input_data_file=path_to_datafile, #The data file\n",
    "                    input_metadata_file=path_to_metadatafile, #The metadata file\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "As can be seen in the printed logs, there is a lot going on when importing the data. That is because tests are applied on your data to check for gaps, and mismatches between data and metadata. \n",
    "\n",
    "We can now inspect the ´dataset´ further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## The attributes\n",
    "\n",
    "The attributes are holding the data of the dataset. Here we present some attributes that can be usefull to inspect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "All classes in the MetObs-toolkit have a ´get_info´ methods that prints out an overview of its content.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "* ´Dataset.obstypes` : A collection of ´Obstypes´ that are known. These observationtypes describe a measurable quantity, and its corresponding units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.obstypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note! The known obstypes are NOT the obstypes for which there are observations.\n",
    "#To get the obstypes for which there are observations, use:\n",
    "dataset.present_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "* ´Dataset.template´: A template class, that is automatically set up by using the template file. This is only used when data is imported from a file. It has no further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = dataset.template\n",
    "\n",
    "template.get_info() # Prints out how the template maps raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "* ´dataset.df´: A pandas DataFrame holding all the observation records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "* ´dataset.metadf´: A pandas DataFrame holding all the metadata of the stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.metadf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Station class\n",
    "\n",
    "The stationclass is a representatio of a station. A station holds the following:\n",
    "\n",
    "* *sensordata*: Timeseries of an observation type. A station can hold multiple sensordata, one for each sensor. \n",
    "* *site*: Each station has a ´Site´ attribute, that holds the information on the location of the station. Metadata related to the station is also stored here. \n",
    "* *modeldata*: In addition to the observations, modeldata timeseries representing the station can be stored. In pracktice, if one would download ERA5 data (using the MetObs-toolkit), the timeseries are stored as modeldata in the Station.\n",
    "\n",
    "\n",
    "To select a station, one can use the *name* of the station, which is assumed to be unique for each station."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "All the methods and attributes that are present in the ´Dataset´ are also applicable on the ´Station´! Thus if your script works on Dataset-level, it also works on station-level. \n",
    "\n",
    "\n",
    "Only the ´Dataset.sync_records()´, ´Dataset.buddy_check()´, and trivial Dataset-only methods (i.g. ´Dataset.get_station()´) are not defined for Stations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a station\n",
    "your_station = dataset.get_station('vlinder02')\n",
    "\n",
    "#Print out some details\n",
    "your_station.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the attributes of the station\n",
    "\n",
    "#Print out info on the Site of the station:\n",
    "your_station.site.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All observational data is stored as SensorData\n",
    "\n",
    "print(your_station.sensordata)\n",
    "\n",
    "# More convenient is to use the pandas dataframe representations,\n",
    "# similar as with the Dataset\n",
    "\n",
    "your_station.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or the metadata for this singel station\n",
    "your_station.metadf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Plotting timeseries\n",
    "\n",
    "Plotting the timeseries can be simply done by using the ´make_plot()´ method, on a ´Dataset´ or a ´Station´."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.make_plot(obstype='temp', #Which observation type to plot. (See dataset.present_observations)\n",
    "                  colorby='station', #if 'station', each station will be a different color\n",
    "                  show_outliers=True,\n",
    "                  show_gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also plot a single station\n",
    "your_station.make_plot(obstype='humidity',\n",
    "                       colorby='label') #If 'label', the colors are based on the status/label of an observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Common usecases\n",
    "\n",
    "Here a collection of common usecases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Resampling time resolution\n",
    "\n",
    "It is common to change or alter the time resolution of your observations. This is often applied when:\n",
    "\n",
    "* the data amount is to big, and the present time resolution is not required for the analysis.\n",
    "* sensor do not have the same time resolution. (i.g. temperature is measured every 5 minutes, but precipitation is measured each hour.)\n",
    "* Observations are not sychronized over multiple stations. This is a special case of resampling, since there is also a synchronization required.\n",
    "\n",
    "It is recommendad to set the target time resolution, in the beginning of your pipeline! \n",
    "\n",
    "In the MetObs-toolkit you can resample by using the ´resample()´ method on a ´Dataset´ or ´Station´. By doing so, the toolkit will construct a set of target timestamps (in the new resolution), and will map the raw timestamps to the new target timestamps. There is no interpolation applied! \n",
    "\n",
    "In order to construct the mapping of the old timestamps to the target timestamps, a tollerance is used. The neirest timestamp is tested if it is within the tolerance of the target timestamp. If this test is not succecsfull, no record could be assigned to the target timestamp and thus a gap is created. Thus by increasing the *shift_tolerance*, the resampling method will have more mapped timestamps thus less gaps but at the cost of less accurate timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_dataset = metobs_toolkit.Dataset()\n",
    "#Load the data (raw data has 5 min resolution)\n",
    "hourly_dataset.import_data_from_file(\n",
    "                    template_file=path_to_templatefile, #The template file\n",
    "                    input_data_file=path_to_datafile, #The data file\n",
    "                    input_metadata_file=path_to_metadatafile, #The metadata file\n",
    "                    )\n",
    "#Resample to 1 hour resolution\n",
    "hourly_dataset.resample(target_freq='1h', #Target frequency is set to 1 hour\n",
    "                        target_obstype=None, #if None, all present observations are resampled\n",
    "                        shift_tolerance='4min', #The maximum shift allowd for a timestamp\n",
    "                        origin_simplify_tolerance='3min') # The maximum shift for the origin, to get a simplified origin\n",
    "\n",
    "# You can verify that the resolution is hourl by inspecing the df attribute\n",
    "hourly_dataset.df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Dataframe of one observationtype\n",
    "\n",
    "The ´Dataset.df` and ´Station.df´ returns a pandas dataframe with a so calld Multi-Index. That is because the combination of [´timestamp´, ´observationtype´, 'stationname´] defines an observation, thus the use of the Multi-Index. \n",
    "\n",
    "We are aware that working with Multi-Indexed dataframes can be challenging, thus an example on how to convert a multiindex dataframe to a regular-indexed dataframe. \n",
    "\n",
    "Be aware that removing (or reducing) the Multi-Index, is always a subsetting or approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset to only temperatures (=subsetting)\n",
    "\n",
    "temperatures = dataset.df.xs(key='temp', \n",
    "                             level='obstype', #the level of the index ('datetime', 'name' or 'obstype')\n",
    "                             drop_level=True)\n",
    "\n",
    "#You can see that the index now only has 2-levels:\n",
    "temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we assume that all the temperature observations over all the stations have the same\n",
    "#set of timestamps (typical after resampling! ), we can create a dataframe with all stations represented by columns.\n",
    "\n",
    "temperatures_wide = (dataset.df\n",
    "                    #first subset to temperatures\n",
    "                    .xs(key='temp', \n",
    "                            level='obstype', #the level of the index ('datetime', 'name' or 'obstype')\n",
    "                            drop_level=True)\n",
    "                    #Convert a index level to columns (unstacking)\n",
    "                    .unstack(level='name'))\n",
    "temperatures_wide\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you are only interested in the value, you can select them:\n",
    "temperatures_wide['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Quality control\n",
    "\n",
    "For an introduction to Quality Control, we refer to the **LINK** ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Extracting data from Google Earth Engine\n",
    "\n",
    "For an introduction to extracting data for GEE, we refer to the **LINK** ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Filling gaps\n",
    "\n",
    "For an introduction to filling gaps, we refer to the **LINK** ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Analysis \n",
    "\n",
    "For an introduction to analysing your dataset, we refer to the **LINK** ."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
